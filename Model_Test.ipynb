{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066a9042",
   "metadata": {},
   "source": [
    "# Topic Modeling and Latent Dirichlet Allocation (LDA) -\n",
    "## ADA Project Milestone 2\n",
    "\n",
    "Trying out the following tutorial on the quotebank2016 data set.\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67e2f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ce3fa",
   "metadata": {},
   "source": [
    "## Part 1: Get & Parse the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b1bf0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 0\n",
      "Processing chunk with 10000 rows\n",
      "                quoteID                                          quotation  \\\n",
      "0     2016-12-26-000040  [ ] and Chris [ Jones ] were in there a lot an...   \n",
      "1     2016-07-31-000006  [ And ] I don't know if we have enough time to...   \n",
      "2     2016-09-06-000292  ... I feel like I was champion long before I l...   \n",
      "3     2016-07-11-000226  [ I ] mmigration has been and continues to be ...   \n",
      "4     2016-05-26-000371  [ It is ] the process of understanding what ki...   \n",
      "...                 ...                                                ...   \n",
      "9995  2016-08-06-031503  It's a great way to take students out into the...   \n",
      "9996  2016-09-01-065324  It's a happy accident, to me, that nothing has...   \n",
      "9997  2016-07-27-070749  It's a joyous occasion and I'd like to think a...   \n",
      "9998  2016-08-15-052447  It's a little bit better than yesterday in som...   \n",
      "9999  2016-05-04-057877  It's a little bit bittersweet for me because T...   \n",
      "\n",
      "              speaker                                               qids  \\\n",
      "0           Andy Reid           [Q2622812, Q27830815, Q470738, Q4761219]   \n",
      "1           Mike Howe                                         [Q6847325]   \n",
      "2                None                                                 []   \n",
      "3     Hillary Clinton                                            [Q6294]   \n",
      "4       Bruce Maxwell                                        [Q26129591]   \n",
      "...               ...                                                ...   \n",
      "9995   Greg Schneider                                         [Q5606223]   \n",
      "9996       Adam Brody                                          [Q294372]   \n",
      "9997      Bill Miller  [Q14951130, Q16187209, Q16732604, Q19325928, Q...   \n",
      "9998             None                                                 []   \n",
      "9999     Kevin Molino                                         [Q6397020]   \n",
      "\n",
      "                    date  numOccurrences  \\\n",
      "0    2016-12-26 20:05:00               1   \n",
      "1    2016-07-31 08:22:12               2   \n",
      "2    2016-09-06 20:54:45               2   \n",
      "3    2016-07-11 17:26:06               1   \n",
      "4    2016-05-26 15:21:37               1   \n",
      "...                  ...             ...   \n",
      "9995 2016-08-06 11:00:00               1   \n",
      "9996 2016-09-01 15:12:37               1   \n",
      "9997 2016-07-27 02:43:37               1   \n",
      "9998 2016-08-15 11:00:00               1   \n",
      "9999 2016-05-04 20:01:00               2   \n",
      "\n",
      "                                                 probas  \\\n",
      "0     [[Andy Reid, 0.9432], [None, 0.0541], [Trevor ...   \n",
      "1                 [[Mike Howe, 0.7118], [None, 0.2882]]   \n",
      "2               [[None, 0.6877], [John Waters, 0.3123]]   \n",
      "3           [[Hillary Clinton, 0.9025], [None, 0.0975]]   \n",
      "4             [[Bruce Maxwell, 0.8178], [None, 0.1822]]   \n",
      "...                                                 ...   \n",
      "9995         [[Greg Schneider, 0.7712], [None, 0.2288]]   \n",
      "9996  [[Adam Brody, 0.4867], [None, 0.438], [Leighto...   \n",
      "9997              [[Bill Miller, 0.784], [None, 0.216]]   \n",
      "9998          [[None, 0.6434], [Larry Richard, 0.3566]]   \n",
      "9999  [[Kevin Molino, 0.8693], [None, 0.1281], [Cris...   \n",
      "\n",
      "                                                   urls phase  \n",
      "0     [http://www.kcchiefs.com/news/article-2/How-a-...     E  \n",
      "1     [http://www.peninsuladailynews.com/apps/pbcs.d...     E  \n",
      "2     [http://onlineathens.com/breaking-news/2016-09...     E  \n",
      "3     [http://www.breitbart.com/tech/2016/07/11/hill...     E  \n",
      "4     [http://www.scout.com/mlb/athletics/story/1673...     E  \n",
      "...                                                 ...   ...  \n",
      "9995  [http://greensburgdailynews.com/news/local_new...     E  \n",
      "9996  [http://www.huffingtonpost.com/2016/09/01/adam...     E  \n",
      "9997  [http://wabi.tv/2016/07/26/22nd-annual-old-fas...     E  \n",
      "9998  [http://iberianet.com/news/still-rescues-in-th...     E  \n",
      "9999  [http://orlandosentinel.com/sports/orlando-cit...     E  \n",
      "\n",
      "[10000 rows x 9 columns]\n",
      "                                              quotation\n",
      "0     [ ] and Chris [ Jones ] were in there a lot an...\n",
      "1     [ And ] I don't know if we have enough time to...\n",
      "2     ... I feel like I was champion long before I l...\n",
      "3     [ I ] mmigration has been and continues to be ...\n",
      "4     [ It is ] the process of understanding what ki...\n",
      "...                                                 ...\n",
      "9995  It's a great way to take students out into the...\n",
      "9996  It's a happy accident, to me, that nothing has...\n",
      "9997  It's a joyous occasion and I'd like to think a...\n",
      "9998  It's a little bit better than yesterday in som...\n",
      "9999  It's a little bit bittersweet for me because T...\n",
      "\n",
      "[10000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# TESTING DATA\n",
    "\n",
    "# Read the first 10000 quotations and store them in a datafram\n",
    "with pd.read_json('Quotebank/quotes-2016.json.bz2', lines=True, compression='bz2', chunksize=10000) as df_reader:\n",
    "    i = 0\n",
    "    for chunk in df_reader:\n",
    "        print(f\"Chunk: {i}\")\n",
    "        test_df = chunk\n",
    "        i += 1\n",
    "        break\n",
    "        \n",
    "print(f'Processing chunk with {len(test_df)} rows')\n",
    "print(test_df)\n",
    "data_quotes = test_df[['quotation']]\n",
    "print(data_quotes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f29b4ab9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           quotation  index\n",
      "0  The public is certainly siding with McGregor b...      0\n",
      "1  I cannot fathom why the Administration would p...      1\n",
      "2  We will continue developing ballistic missile-...      2\n",
      "3  Hey Lee / Your short game's good / but your lo...      3\n",
      "4  There are many, many people who invest and par...      4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8m/8wzqh5x904s70_r9w7h9_zsw0000gn/T/ipykernel_22655/1019052835.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text['index'] = data_text.index\n"
     ]
    }
   ],
   "source": [
    "# TRAINING DATA\n",
    "\n",
    "data = pd.read_csv('NYTimes/NYTimes_topic_modeling_training.zip', compression='zip')\n",
    "data_text = data[['quotation']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "print(documents.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7a879",
   "metadata": {},
   "source": [
    "## Part 2: Tokenization & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79264e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/simonspangenberg/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2018)\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75ddf8",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eaa12e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize: Lemmatization technique is like stemming. The output we will get after lemmatization is called \n",
    "# ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will \n",
    "# be getting a valid word that means the same thing.\n",
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ac43e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemmatize: Stemming is a technique used to extract the base form of the words by removing affixes from them. \n",
    "# It is just like cutting down the branches of a tree to its stems. \n",
    "# For example, the stem of the words eating, eats, eaten is eat.\n",
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3cd3e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions we will use\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        my_stop_words = STOPWORDS.union(set(['know', 'think','want','go','get','need','thing','like','feel']))\n",
    "        if token not in my_stop_words and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5897c4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original quotation: \n",
      "['Tomorrow,', 'whatever', 'the', 'conditions', 'are,', 'I', 'think', 'we', 'should', 'be', 'confident', 'that', 'we', 'can', 'score', 'points', 'and,', 'hopefully,', 'as', 'last', 'year,', 'we', 'can', 'have', 'both', 'cars', 'in', 'the', 'points', 'and', 'help', 'the', 'team', 'in', 'that', 'respect.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['tomorrow', 'condit', 'confid', 'score', 'point', 'hope', 'year', 'car', 'point', 'help', 'team', 'respect']\n"
     ]
    }
   ],
   "source": [
    "sample = documents[documents.index == 4310].values[0][0]\n",
    "\n",
    "print('original quotation: ')\n",
    "words = []\n",
    "for word in sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efa11569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [public, certain, side, mcgregor, valu, wisegu...\n",
      "1     [fathom, administr, pursu, cours, signal, chan...\n",
      "2     [continu, develop, ballist, missil, defens, te...\n",
      "3               [short, game, good, long, game, better]\n",
      "4     [peopl, invest, particip, level, time, effort,...\n",
      "5                                              [cyborg]\n",
      "6     [transform, live, present, grow, challeng, hum...\n",
      "7                  [presid, trump, hasn, decis, moment]\n",
      "8     [insati, greed, check, wealth, aggreg, hand, r...\n",
      "9     [talk, time, end, sexism, form, discrimin, bel...\n",
      "10               [group, look, work, civil, right, len]\n",
      "11    [constant, learn, connect, compani, come, see,...\n",
      "12    [stakehold, talk, industri, size, biggest, sma...\n",
      "13        [grow, strong, women, return, destroy, world]\n",
      "14    [think, look, come, putter, laugh, go, famous,...\n",
      "15    [role, cleaner, effici, fossil, fuel, nuclear,...\n",
      "16         [shine, strong, spotlight, financ, campaign]\n",
      "17    [year, inconsist, analyz, cost, benefit, regul...\n",
      "18                       [goal, pitcher, number, lower]\n",
      "19    [rich, peopl, live, longer, peopl, life, expec...\n",
      "20                                 [olymp, stori, good]\n",
      "21         [climat, chang, tend, increas, exist, inequ]\n",
      "22    [weed, manag, polici, perri, cast, vision, sel...\n",
      "23    [fast, electr, vehicl, scenario, world, go, bu...\n",
      "24    [polici, place, prevent, confus, offici, censu...\n",
      "25    [go, depart, educ, expect, governmentwid, pres...\n",
      "26    [protect, consum, prioriti, len, evalu, catast...\n",
      "27                                   [guy, pretti, guy]\n",
      "28    [attempt, hack, ministri, websit, legal, compl...\n",
      "29                                [yield, admir, grace]\n",
      "Name: quotation, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Process all the quotations\n",
    "processed_docs = documents['quotation'].map(preprocess)\n",
    "print(processed_docs.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa6c2a6",
   "metadata": {},
   "source": [
    "## Part 3: Create a Dictionary of Words & Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "09e2ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 certain\n",
      "1 fight\n",
      "2 mcgregor\n",
      "3 public\n",
      "4 side\n",
      "5 valu\n",
      "6 wiseguy\n",
      "7 administr\n",
      "8 align\n",
      "9 american\n",
      "10 autocrat\n",
      "9428\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary from ‘processed_docs’ df containing the number of times a word appears in the training set\n",
    "count = 0\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "300f5acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(113, 1),\n",
       " (170, 1),\n",
       " (199, 1),\n",
       " (247, 1),\n",
       " (309, 1),\n",
       " (374, 1),\n",
       " (377, 1),\n",
       " (481, 2),\n",
       " (656, 1),\n",
       " (800, 1),\n",
       " (1209, 1)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter tokens that appear: >15 times, more than 0.5 documents. \n",
    "# Then keep only the most frequent 100000 tokens. \n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9cde024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 37 (\"effort\") appears 1 time.\n",
      "Word 379 (\"combat\") appears 1 time.\n",
      "Word 380 (\"rais\") appears 1 time.\n",
      "Word 381 (\"revenu\") appears 1 time.\n",
      "Word 382 (\"way\") appears 2 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_100 = bow_corpus[100]\n",
    "for i in range(len(bow_doc_100)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_100[i][0], \n",
    "                                                     dictionary[bow_doc_100[i][0]], \n",
    "                                                     bow_doc_100[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1699acd2",
   "metadata": {},
   "source": [
    "## Part 4: Create a TF-IDF Model\n",
    "\n",
    "TF-IDF stands for Term Frequency Inverse Document Frequency of records. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus (data-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7bf04de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ab2b6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2fa473f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.29397709448002163),\n",
      " (1, 0.3039120552343348),\n",
      " (2, 0.26511944366641427),\n",
      " (3, 0.803808648364201),\n",
      " (4, 0.32375649492983805)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e94e2d4",
   "metadata": {},
   "source": [
    "## Part 5: LDA\n",
    "\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’\n",
    "\n",
    "Topic modeling is a way of abstract modeling to discover the abstract ‘topics’ that occur in the collections of documents. The idea is that we will perform unsupervised classification on different documents, which find some natural groups in topics. We can answer the following question using topic modeling.\n",
    "\n",
    "    What is the topic/main idea of the document?\n",
    "    Given a document, can we find another document with a similar topic?\n",
    "    How do topics field change over time?\n",
    "\n",
    "Latent Dirichlet allocation is one of the most popular methods for performing topic modeling. Each document consists of various words and each topic can be associated with some words. The aim behind the LDA to find topics that the document belongs to, on the basis of words contains in it. It assumes that documents with similar topics will use a similar group of words. This enables the documents to map the probability distribution over latent topics and topics are probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6cae21",
   "metadata": {},
   "source": [
    "### Using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6cbceaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=4, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ef4e193b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.014*\"go\" + 0.013*\"right\" + 0.012*\"climat\" + 0.012*\"play\" + 0.011*\"game\" + 0.010*\"look\" + 0.010*\"chang\" + 0.009*\"state\" + 0.009*\"see\" + 0.009*\"year\"\n",
      "Topic: 1 \n",
      "Words: 0.029*\"go\" + 0.027*\"peopl\" + 0.017*\"work\" + 0.013*\"trump\" + 0.013*\"come\" + 0.009*\"happen\" + 0.009*\"thing\" + 0.009*\"talk\" + 0.008*\"presid\" + 0.008*\"team\"\n",
      "Topic: 2 \n",
      "Words: 0.016*\"time\" + 0.014*\"chang\" + 0.009*\"take\" + 0.008*\"climat\" + 0.008*\"littl\" + 0.008*\"world\" + 0.008*\"get\" + 0.007*\"busi\" + 0.007*\"kind\" + 0.007*\"work\"\n",
      "Topic: 3 \n",
      "Words: 0.016*\"peopl\" + 0.015*\"good\" + 0.013*\"year\" + 0.010*\"play\" + 0.008*\"compani\" + 0.007*\"great\" + 0.007*\"game\" + 0.007*\"start\" + 0.007*\"better\" + 0.007*\"make\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c038bcf",
   "metadata": {},
   "source": [
    "### Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a9960698",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=4, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "47046029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.014*\"go\" + 0.010*\"come\" + 0.008*\"peopl\" + 0.007*\"good\" + 0.007*\"work\" + 0.006*\"time\" + 0.006*\"care\" + 0.006*\"say\" + 0.006*\"chang\" + 0.006*\"trump\"\n",
      "Topic: 1 Word: 0.008*\"year\" + 0.008*\"go\" + 0.007*\"peopl\" + 0.007*\"problem\" + 0.007*\"better\" + 0.006*\"player\" + 0.006*\"game\" + 0.006*\"team\" + 0.006*\"take\" + 0.006*\"focus\"\n",
      "Topic: 2 Word: 0.009*\"look\" + 0.008*\"time\" + 0.007*\"peopl\" + 0.007*\"happen\" + 0.007*\"go\" + 0.006*\"get\" + 0.006*\"talk\" + 0.005*\"year\" + 0.005*\"chang\" + 0.005*\"point\"\n",
      "Topic: 3 Word: 0.009*\"peopl\" + 0.008*\"go\" + 0.008*\"play\" + 0.007*\"right\" + 0.005*\"thing\" + 0.005*\"world\" + 0.005*\"deal\" + 0.005*\"import\" + 0.005*\"water\" + 0.005*\"work\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256f7f1",
   "metadata": {},
   "source": [
    "## Part 6: Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "85d5fc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [public, certain, side, mcgregor, valu, wisegu...\n",
      "1    [fathom, administr, pursu, cours, signal, chan...\n",
      "2    [continu, develop, ballist, missil, defens, te...\n",
      "Name: quotation, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Should be classified in a sports topic\n",
    "print(processed_docs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce46ac1f",
   "metadata": {},
   "source": [
    "### On LDA BOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f23f3cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8894662261009216\t \n",
      "Topic: 0.016*\"peopl\" + 0.015*\"good\" + 0.013*\"year\" + 0.010*\"play\" + 0.008*\"compani\" + 0.007*\"great\" + 0.007*\"game\" + 0.007*\"start\" + 0.007*\"better\" + 0.007*\"make\"\n",
      "\n",
      "Score: 0.0373750664293766\t \n",
      "Topic: 0.016*\"time\" + 0.014*\"chang\" + 0.009*\"take\" + 0.008*\"climat\" + 0.008*\"littl\" + 0.008*\"world\" + 0.008*\"get\" + 0.007*\"busi\" + 0.007*\"kind\" + 0.007*\"work\"\n",
      "\n",
      "Score: 0.036617159843444824\t \n",
      "Topic: 0.014*\"go\" + 0.013*\"right\" + 0.012*\"climat\" + 0.012*\"play\" + 0.011*\"game\" + 0.010*\"look\" + 0.010*\"chang\" + 0.009*\"state\" + 0.009*\"see\" + 0.009*\"year\"\n",
      "\n",
      "Score: 0.036541566252708435\t \n",
      "Topic: 0.029*\"go\" + 0.027*\"peopl\" + 0.017*\"work\" + 0.013*\"trump\" + 0.013*\"come\" + 0.009*\"happen\" + 0.009*\"thing\" + 0.009*\"talk\" + 0.008*\"presid\" + 0.008*\"team\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[0]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbad2c8",
   "metadata": {},
   "source": [
    "### On LDA TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7a16ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8871537446975708\t \n",
      "Topic: 0.009*\"look\" + 0.008*\"time\" + 0.007*\"peopl\" + 0.007*\"happen\" + 0.007*\"go\" + 0.006*\"get\" + 0.006*\"talk\" + 0.005*\"year\" + 0.005*\"chang\" + 0.005*\"point\"\n",
      "\n",
      "Score: 0.03781189024448395\t \n",
      "Topic: 0.008*\"year\" + 0.008*\"go\" + 0.007*\"peopl\" + 0.007*\"problem\" + 0.007*\"better\" + 0.006*\"player\" + 0.006*\"game\" + 0.006*\"team\" + 0.006*\"take\" + 0.006*\"focus\"\n",
      "\n",
      "Score: 0.037610866129398346\t \n",
      "Topic: 0.014*\"go\" + 0.010*\"come\" + 0.008*\"peopl\" + 0.007*\"good\" + 0.007*\"work\" + 0.006*\"time\" + 0.006*\"care\" + 0.006*\"say\" + 0.006*\"chang\" + 0.006*\"trump\"\n",
      "\n",
      "Score: 0.03742348402738571\t \n",
      "Topic: 0.009*\"peopl\" + 0.008*\"go\" + 0.008*\"play\" + 0.007*\"right\" + 0.005*\"thing\" + 0.005*\"world\" + 0.005*\"deal\" + 0.005*\"import\" + 0.005*\"water\" + 0.005*\"work\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[0]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d64ad1",
   "metadata": {},
   "source": [
    "## Part 7: Testing with Quotebank Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8391be0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A White House National Space Council can be useful if the president wants one and is willing to back it up when other White House offices, like the Office of Management and Budget, balk at its recommendations. If not, then it is a waste of resources,\n"
     ]
    }
   ],
   "source": [
    "quotebank_example = data_quotes['quotation'][7000]\n",
    "print(quotebank_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "683e9b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5477136373519897\t Topic: 0.029*\"go\" + 0.027*\"peopl\" + 0.017*\"work\" + 0.013*\"trump\" + 0.013*\"come\"\n",
      "Score: 0.42258450388908386\t Topic: 0.016*\"peopl\" + 0.015*\"good\" + 0.013*\"year\" + 0.010*\"play\" + 0.008*\"compani\"\n",
      "Score: 0.014941971749067307\t Topic: 0.014*\"go\" + 0.013*\"right\" + 0.012*\"climat\" + 0.012*\"play\" + 0.011*\"game\"\n",
      "Score: 0.014759933575987816\t Topic: 0.016*\"time\" + 0.014*\"chang\" + 0.009*\"take\" + 0.008*\"climat\" + 0.008*\"littl\"\n"
     ]
    }
   ],
   "source": [
    "bow_vector = dictionary.doc2bow(preprocess(quotebank_example))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacff5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
