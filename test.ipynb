{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066a9042",
   "metadata": {},
   "source": [
    "# Topic Modeling and Latent Dirichlet Allocation (LDA) -\n",
    "## ADA Project Milestone 2\n",
    "\n",
    "Trying out the following tutorial on the quotebank2016 data set.\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "67e2f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from gensim import corpora, models\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ce3fa",
   "metadata": {},
   "source": [
    "## Part 1: Get & Parse the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "6b1bf0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk: 0\n",
      "Processing chunk with 1000000 rows\n",
      "                                                quotation\n",
      "0       [ ] and Chris [ Jones ] were in there a lot an...\n",
      "1       [ And ] I don't know if we have enough time to...\n",
      "2       ... I feel like I was champion long before I l...\n",
      "3       [ I ] mmigration has been and continues to be ...\n",
      "4       [ It is ] the process of understanding what ki...\n",
      "...                                                   ...\n",
      "999995  The views from the existing restaurant here ar...\n",
      "999996  The Viking way has been working. It's a team t...\n",
      "999997  the vivid combination of the intriguing, the s...\n",
      "999998  The Vuelta is a race I really enjoy and one wh...\n",
      "999999     the way he kept bringing it back to Americans.\n",
      "\n",
      "[1000000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# TESTING DATA\n",
    "\n",
    "# Read the first 10000 quotations and store them in a datafram\n",
    "with pd.read_json('Quotebank/quotes-2016.json.bz2', lines=True, compression='bz2', chunksize=1000000) as df_reader:\n",
    "    i = 0\n",
    "    for chunk in df_reader:\n",
    "        print(f\"Chunk: {i}\")\n",
    "        test_df = chunk\n",
    "        i += 1\n",
    "        break\n",
    "        \n",
    "print(f'Processing chunk with {len(test_df)} rows')\n",
    "data_quotes = test_df[['quotation']]\n",
    "print(data_quotes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f29b4ab9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ada/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3441: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       headline_text  index\n",
      "0  aba decides against community broadcasting lic...      0\n",
      "1     act fire witnesses must be aware of defamation      1\n",
      "2     a g calls for infrastructure protection summit      2\n",
      "3           air nz staff in aust strike for pay rise      3\n",
      "4      air nz strike to affect australian travellers      4\n"
     ]
    }
   ],
   "source": [
    "# TRAINING DATA\n",
    "\n",
    "data = pd.read_csv('archive.zip', error_bad_lines=False, compression='zip')\n",
    "data_text = data[['headline_text']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text\n",
    "print(documents.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c7a879",
   "metadata": {},
   "source": [
    "## Part 2: Tokenization & Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "79264e8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/simonspangenberg/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2018)\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb75ddf8",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "eaa12e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize: Lemmatization technique is like stemming. The output we will get after lemmatization is called \n",
    "# ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will \n",
    "# be getting a valid word that means the same thing.\n",
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3ac43e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemmatize: Stemming is a technique used to extract the base form of the words by removing affixes from them. \n",
    "# It is just like cutting down the branches of a tree to its stems. \n",
    "# For example, the stem of the words eating, eats, eaten is eat.\n",
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3cd3e844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions we will use\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5897c4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original quotation: \n",
      "['so', 'far,', \"they've\", 'succeeded.']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['succeed']\n"
     ]
    }
   ],
   "source": [
    "sample = data_quotes[data_quotes.index == 4310].values[0][0]\n",
    "\n",
    "print('original quotation: ')\n",
    "words = []\n",
    "for word in sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "efa11569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [chris, jone, tamba, hali, get, good, push, ki...\n",
      "1                         [know, time, sell, communiti]\n",
      "2     [feel, like, champion, long, leav, cedar, shoa...\n",
      "3                     [mmigrat, continu, good, economi]\n",
      "4               [process, understand, kind, hitter, go]\n",
      "5     [flexibl, term, pay, competit, medic, communit...\n",
      "6           [leftist, reorgan, capit, accumul, reorgan]\n",
      "7     [malia, know, go, plan, famili, feel, comfort,...\n",
      "8     [monterey, support, public, transpar, scienc, ...\n",
      "9               [silver, come, suit, felt, good, posit]\n",
      "10    [fail, acknowledg, basic, biolog, differ, fact...\n",
      "11                          [abort, pill, show, unwork]\n",
      "12    [busi, schedul, excus, know, game, month, half...\n",
      "13    [imperfect, report, thought, express, prophet,...\n",
      "14                 [basic, tenet, fair, honesti, share]\n",
      "15                                              [hesit]\n",
      "16    [omen, carri, unwant, pregnanc, term, like, li...\n",
      "17    [weber, kind, go, raquo, puck, daddi, hour, su...\n",
      "18    [acquir, allow, immedi, entri, player, platinu...\n",
      "19                       [worley, give, chanc, ballgam]\n",
      "20    [say, love, famili, think, ask, time, pass, cl...\n",
      "21                        [dead, explos, possibl, drug]\n",
      "22    [alleg, underli, matter, implic, cosbi, sexual...\n",
      "23    [indian, crew, member, rescu, effort, secur, r...\n",
      "24    [individu, wrap, baggi, marijuana, weigh, appr...\n",
      "25                                      [grow, tobacco]\n",
      "26        [year, joke, simpson, propheci, donald, prez]\n",
      "27                                                   []\n",
      "28    [year, go, seven, male, dog, north, pole, sexi...\n",
      "29                            [day, elect, break, news]\n",
      "Name: quotation, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Process all the quotations\n",
    "processed_docs = data_quotes['quotation'].map(preprocess)\n",
    "print(processed_docs.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa6c2a6",
   "metadata": {},
   "source": [
    "## Part 3: Create a Dictionary of Words & Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "09e2ec9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 allow\n",
      "1 chris\n",
      "2 creat\n",
      "3 get\n",
      "4 good\n",
      "5 guy\n",
      "6 hali\n",
      "7 jone\n",
      "8 kind\n",
      "9 pressur\n",
      "10 push\n",
      "110952\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary from ‘processed_docs’ df containing the number of times a word appears in the training set\n",
    "count = 0\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "300f5acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3697, 1)]"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter tokens that appear: >15 times, more than 0.5 documents. \n",
    "# Then keep only the most frequent 100000 tokens. \n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "9cde024c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 26 (\"go\") appears 1 time.\n",
      "Word 366 (\"money\") appears 1 time.\n",
      "Word 459 (\"team\") appears 1 time.\n",
      "Word 600 (\"claim\") appears 1 time.\n",
      "Word 601 (\"clue\") appears 1 time.\n",
      "Word 602 (\"collect\") appears 1 time.\n",
      "Word 603 (\"forc\") appears 1 time.\n",
      "Word 604 (\"identifi\") appears 1 time.\n",
      "Word 605 (\"inspector\") appears 1 time.\n",
      "Word 606 (\"ransom\") appears 1 time.\n",
      "Word 607 (\"scooter\") appears 1 time.\n",
      "Word 608 (\"special\") appears 1 time.\n",
      "Word 609 (\"task\") appears 1 time.\n",
      "Word 610 (\"victim\") appears 1 time.\n",
      "Word 611 (\"vital\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_100 = bow_corpus[100]\n",
    "for i in range(len(bow_doc_100)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_100[i][0], \n",
    "                                                     dictionary[bow_doc_100[i][0]], \n",
    "                                                     bow_doc_100[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1699acd2",
   "metadata": {},
   "source": [
    "## Part 4: Create a TF-IDF Model\n",
    "\n",
    "TF-IDF stands for Term Frequency Inverse Document Frequency of records. It can be defined as the calculation of how relevant a word in a series or corpus is to a text. The meaning increases proportionally to the number of times in the text a word appears but is compensated by the word frequency in the corpus (data-set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "7bf04de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "ab2b6e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2fa473f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.2932805456631883),\n",
      " (1, 0.4179816619033531),\n",
      " (2, 0.27942714023179815),\n",
      " (3, 0.2373521144919461),\n",
      " (4, 0.18662502799451475),\n",
      " (5, 0.25757101461800186),\n",
      " (6, 0.4669767314584352),\n",
      " (7, 0.25032592036130136),\n",
      " (8, 0.3374073392312525),\n",
      " (9, 0.3303279030832013)]\n"
     ]
    }
   ],
   "source": [
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e94e2d4",
   "metadata": {},
   "source": [
    "## Part 5: LDA\n",
    "\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’\n",
    "\n",
    "Topic modeling is a way of abstract modeling to discover the abstract ‘topics’ that occur in the collections of documents. The idea is that we will perform unsupervised classification on different documents, which find some natural groups in topics. We can answer the following question using topic modeling.\n",
    "\n",
    "    What is the topic/main idea of the document?\n",
    "    Given a document, can we find another document with a similar topic?\n",
    "    How do topics field change over time?\n",
    "\n",
    "Latent Dirichlet allocation is one of the most popular methods for performing topic modeling. Each document consists of various words and each topic can be associated with some words. The aim behind the LDA to find topics that the document belongs to, on the basis of words contains in it. It assumes that documents with similar topics will use a similar group of words. This enables the documents to map the probability distribution over latent topics and topics are probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6cae21",
   "metadata": {},
   "source": [
    "### Using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6cbceaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "ef4e193b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.101*\"year\" + 0.020*\"time\" + 0.017*\"month\" + 0.016*\"start\" + 0.016*\"week\" + 0.014*\"final\" + 0.014*\"second\" + 0.013*\"come\" + 0.013*\"day\" + 0.012*\"half\"\n",
      "Topic: 1 \n",
      "Words: 0.013*\"communiti\" + 0.012*\"work\" + 0.011*\"busi\" + 0.011*\"continu\" + 0.010*\"help\" + 0.010*\"need\" + 0.009*\"support\" + 0.008*\"govern\" + 0.008*\"develop\" + 0.008*\"school\"\n",
      "Topic: 2 \n",
      "Words: 0.023*\"state\" + 0.017*\"trump\" + 0.015*\"parti\" + 0.015*\"presid\" + 0.015*\"polit\" + 0.014*\"support\" + 0.013*\"elect\" + 0.013*\"countri\" + 0.012*\"vote\" + 0.011*\"american\"\n",
      "Topic: 3 \n",
      "Words: 0.019*\"chang\" + 0.019*\"world\" + 0.014*\"look\" + 0.013*\"forward\" + 0.013*\"women\" + 0.011*\"citi\" + 0.010*\"histori\" + 0.009*\"work\" + 0.008*\"event\" + 0.008*\"film\"\n",
      "Topic: 4 \n",
      "Words: 0.013*\"case\" + 0.011*\"offic\" + 0.011*\"care\" + 0.010*\"polic\" + 0.009*\"health\" + 0.008*\"public\" + 0.008*\"protect\" + 0.008*\"forc\" + 0.008*\"inform\" + 0.008*\"take\"\n",
      "Topic: 5 \n",
      "Words: 0.025*\"work\" + 0.020*\"money\" + 0.016*\"time\" + 0.016*\"hard\" + 0.015*\"hand\" + 0.014*\"go\" + 0.012*\"get\" + 0.012*\"spend\" + 0.012*\"leav\" + 0.011*\"water\"\n",
      "Topic: 6 \n",
      "Words: 0.038*\"good\" + 0.038*\"play\" + 0.026*\"game\" + 0.025*\"team\" + 0.022*\"think\" + 0.021*\"go\" + 0.015*\"great\" + 0.014*\"player\" + 0.013*\"better\" + 0.012*\"come\"\n",
      "Topic: 7 \n",
      "Words: 0.012*\"term\" + 0.011*\"market\" + 0.010*\"product\" + 0.009*\"long\" + 0.008*\"increas\" + 0.007*\"custom\" + 0.007*\"time\" + 0.007*\"high\" + 0.007*\"price\" + 0.006*\"number\"\n",
      "Topic: 8 \n",
      "Words: 0.076*\"like\" + 0.028*\"love\" + 0.024*\"feel\" + 0.017*\"want\" + 0.016*\"famili\" + 0.012*\"friend\" + 0.012*\"know\" + 0.010*\"time\" + 0.010*\"come\" + 0.010*\"kind\"\n",
      "Topic: 9 \n",
      "Words: 0.076*\"peopl\" + 0.048*\"go\" + 0.046*\"think\" + 0.039*\"know\" + 0.031*\"thing\" + 0.028*\"want\" + 0.020*\"happen\" + 0.019*\"say\" + 0.017*\"come\" + 0.015*\"tell\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c038bcf",
   "metadata": {},
   "source": [
    "### Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a9960698",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "47046029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.006*\"squad\" + 0.005*\"struggl\" + 0.005*\"nice\" + 0.004*\"go\" + 0.004*\"like\" + 0.004*\"hand\" + 0.004*\"nervous\" + 0.004*\"christma\" + 0.004*\"light\" + 0.004*\"excus\"\n",
      "Topic: 1 Word: 0.017*\"play\" + 0.015*\"go\" + 0.014*\"game\" + 0.014*\"think\" + 0.014*\"good\" + 0.012*\"know\" + 0.011*\"team\" + 0.010*\"like\" + 0.010*\"want\" + 0.010*\"thing\"\n",
      "Topic: 2 Word: 0.009*\"peopl\" + 0.008*\"music\" + 0.007*\"stori\" + 0.007*\"like\" + 0.007*\"write\" + 0.006*\"film\" + 0.006*\"song\" + 0.006*\"tell\" + 0.006*\"think\" + 0.006*\"say\"\n",
      "Topic: 3 Word: 0.006*\"go\" + 0.005*\"like\" + 0.005*\"peopl\" + 0.005*\"come\" + 0.005*\"walk\" + 0.005*\"time\" + 0.005*\"open\" + 0.004*\"think\" + 0.004*\"door\" + 0.004*\"road\"\n",
      "Topic: 4 Word: 0.014*\"trump\" + 0.011*\"vote\" + 0.009*\"elect\" + 0.009*\"parti\" + 0.008*\"presid\" + 0.007*\"peopl\" + 0.007*\"donald\" + 0.007*\"clinton\" + 0.006*\"hillari\" + 0.006*\"state\"\n",
      "Topic: 5 Word: 0.007*\"money\" + 0.005*\"peopl\" + 0.005*\"wear\" + 0.005*\"like\" + 0.005*\"go\" + 0.004*\"talk\" + 0.004*\"cancer\" + 0.004*\"think\" + 0.004*\"treat\" + 0.004*\"sick\"\n",
      "Topic: 6 Word: 0.005*\"busi\" + 0.005*\"communiti\" + 0.005*\"need\" + 0.005*\"work\" + 0.005*\"market\" + 0.004*\"year\" + 0.004*\"continu\" + 0.004*\"develop\" + 0.004*\"compani\" + 0.004*\"provid\"\n",
      "Topic: 7 Word: 0.013*\"love\" + 0.009*\"famili\" + 0.008*\"life\" + 0.008*\"peopl\" + 0.007*\"like\" + 0.007*\"live\" + 0.006*\"know\" + 0.006*\"want\" + 0.006*\"friend\" + 0.006*\"thank\"\n",
      "Topic: 8 Word: 0.006*\"question\" + 0.006*\"answer\" + 0.005*\"peopl\" + 0.005*\"problem\" + 0.005*\"kill\" + 0.005*\"go\" + 0.005*\"fight\" + 0.005*\"stop\" + 0.004*\"wors\" + 0.004*\"weapon\"\n",
      "Topic: 9 Word: 0.006*\"govern\" + 0.006*\"issu\" + 0.005*\"case\" + 0.005*\"court\" + 0.004*\"polic\" + 0.004*\"offic\" + 0.004*\"investig\" + 0.004*\"inform\" + 0.004*\"public\" + 0.004*\"state\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7256f7f1",
   "metadata": {},
   "source": [
    "## Part 6: Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "85d5fc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [chris, jone, tamba, hali, get, good, push, ki...\n",
      "1                        [know, time, sell, communiti]\n",
      "2    [feel, like, champion, long, leav, cedar, shoa...\n",
      "Name: quotation, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Should be classified in a sports topic\n",
    "print(processed_docs[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce46ac1f",
   "metadata": {},
   "source": [
    "### On LDA BOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f23f3cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7752967476844788\t \n",
      "Topic: 0.038*\"good\" + 0.038*\"play\" + 0.026*\"game\" + 0.025*\"team\" + 0.022*\"think\" + 0.021*\"go\" + 0.015*\"great\" + 0.014*\"player\" + 0.013*\"better\" + 0.012*\"come\"\n",
      "\n",
      "Score: 0.15195025503635406\t \n",
      "Topic: 0.019*\"chang\" + 0.019*\"world\" + 0.014*\"look\" + 0.013*\"forward\" + 0.013*\"women\" + 0.011*\"citi\" + 0.010*\"histori\" + 0.009*\"work\" + 0.008*\"event\" + 0.008*\"film\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[0]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbad2c8",
   "metadata": {},
   "source": [
    "### On LDA TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e7a16ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6041104197502136\t \n",
      "Topic: 0.017*\"play\" + 0.015*\"go\" + 0.014*\"game\" + 0.014*\"think\" + 0.014*\"good\" + 0.012*\"know\" + 0.011*\"team\" + 0.010*\"like\" + 0.010*\"want\" + 0.010*\"thing\"\n",
      "\n",
      "Score: 0.32311466336250305\t \n",
      "Topic: 0.009*\"peopl\" + 0.008*\"music\" + 0.007*\"stori\" + 0.007*\"like\" + 0.007*\"write\" + 0.006*\"film\" + 0.006*\"song\" + 0.006*\"tell\" + 0.006*\"think\" + 0.006*\"say\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[0]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d64ad1",
   "metadata": {},
   "source": [
    "## Part 7: Testing with Quotebank Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8391be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotebank_example = data_quotes['quotation'][12]\n",
    "print(quotebank_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e9b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = dictionary.doc2bow(preprocess(quotebank_example))\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacff5e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada] *",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
